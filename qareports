#!/bin/bash

set -eu


export PATH="./bin:$PATH"
available_envs="dev|staging|production"


check_dep () {
    if ! which "$1" > /dev/null
    then
      echo "Could not find $1! Please install using your package manager or place its binary in './bin'"
      exit 1
    fi
}


# Check if all tools are installed, do not go on if any is missing
for tool in eksctl terraform ansible kubectl
do
    check_dep $tool
done


qa_help() {
    echo "This is qareports' deploy management tool."
    echo
    echo "Usage: ./qareports $available_envs [command[args]]"
    echo
    echo "Commands:"
    echo "  up             create the entire setup"
    echo "  destroy        destroy the entire setup"
    echo "  list           show all services (e.g. frontend, workers, etc)"
    echo "  upgrade        upgrade the entire setup"
    echo "  ssh node|pod   ssh into a node or a pod"
    echo "  logs [-f] pod  display logs for a given pod, pass -f to keep following"
    echo "  dashboard      pop up a kubernetes dashboard"
    echo "  k              runs kubectl for k8s specific commands"
    echo
    echo "Example:"
    echo "  $ ./qareports production upgrade  # probably the most frequent command"
}

qa_up() {
    echo "Creating $environment"

    if [ $environment == "dev" ]
    then
        echo "It should take a few minutes, after that you should be able to see squad in action at http://192.168.50.10"
        # Set up 2 virtubalboxes and install kubernetes
        check_dep vagrant
        cd ansible
        vagrant up
        ./deploy
        cd ..

        # Get generated kubeconfig
        scp \
            -F ansible/.vagrant/ssh_config \
            -o StrictHostKeyChecking=no \
            master-node:/home/vagrant/.kube/config $KUBECONFIG 

        # Prepare k8s environment
        kubectl apply -f k8s/qareports-dev-namespace.yml
        kubectl apply -f k8s/metrics-server-dev.yml
    else
        echo "Creating terraform dependencies for $environment"
        cd terraform
        ./terraform $environment apply
        exit 0
        # eksctl create cluster -f k8s
        #k apply -f k8s/metrics-server-aws.yml  # fetch metrics from k8s API to feed autoscalers
        #k apply -f k8s/qareports-namespaces.yml  # start job to apply migration, delete when finish
    fi

    sleep $((60 * 2))  # wait about 2 minutes for kube-system pods to get started
    qa_deploy_app
    qa_list
}

qa_destroy() {
    echo "Destroying $environment"
    if [ $environment == "dev" ]
    then
        # Set up 2 virtubalboxes and install kubernetes
        check_dep vagrant
        rm -rf $KUBECONFIG ansible/.vagrant/ssh_config
        cd ansible
        vagrant destroy -f
    else
        # Destroy production/staging
        # eksctl destroy cluster -f k8s/cluster.yml
        echo "Wait 5 minutes to let EKS cluster to finish destroying"
        sleep $((60 * 5))
        cd terraform
        ./terraform destroy
    fi
}

qa_migrate_db() {
    kubectl apply -f k8s/qareports-migration.yml  # start job to apply migration, delete when finish
    kubectl wait --for=condition=complete --timeout=300s job/qareports-migration  # Wait 5 minutes for migration to complete
    if [ $? != 0 ]
    then
        echo "Failed to apply migration:"
        kubectl logs job/qareports-migration
        kubectl delete -f k8s/qareports-migration.yml  # delete failed migration job
        echo "*** ABORTING !!! ***"
        exit 1
    fi

    kubectl delete -f k8s/qareports-migration.yml  # delete completed migration job
}

qa_upgrade() {
    echo "Upgraging $environment"

    qa_deploy_app
    # Check these commands later
    # kubectl set image deployment/frontend www=image:v2               # Rolling update "www" containers of "frontend" deployment, updating the image
    # kubectl rollout history deployment/frontend                      # Check the history of deployments including the revision
    # kubectl rollout undo deployment/frontend                         # Rollback to the previous deployment
    # kubectl rollout undo deployment/frontend --to-revision=2         # Rollback to a specific revision
    # kubectl rollout status -w deployment/frontend                    # Watch rolling update status of "frontend" deployment until completion
    # kubectl rollout restart deployment/frontend                      # Rolling restart of the "frontend" deployment`
}

qa_deploy_app() {
    echo "Deploying $environment"

    kubectl config set-context --current --namespace=qareports-$environment

    # set secrets/environment variables
    if [ $environment == "dev" ]
    then
        # TODO: merge all dev files into a single one
        kubectl apply -f k8s/qareports-secrets-dev.yml
        kubectl apply -f k8s/qareports-ldap-dev.yml
    else
        # TODO: create a "vars" file for prod and staging, then translate them into
        # k8s secrets
        ./scripts/ansible-vault view secrets/${environment}.env | ./scripts/env2secrets.py | kubectl apply -f -
        ./scripts/ansible-vault view secrets/qareports-ldap.yml | kubectl apply -f -
        ./scripts/ansible-vault view secrets/qareports-secret-key.yml | kubectl apply -f -
    fi
    qa_migrate_db
    kubectl apply -f k8s/qareports-web.yml                   # startup frontend deploy
    kubectl apply -f k8s/qareports-worker.yml                # startup worker (no fetching) deploy
    kubectl apply -f k8s/qareports-listener.yml              # startup listener deploy
    kubectl apply -f k8s/qareports-scheduler.yml             # startup scheduler deploy
    kubectl apply -f k8s/qareports-fetch-worker.yml          # startup fetch worker deploy
}

qa_list() {
    echo "Showing services for $environment"
    echo "Non-kubernetes nodes"
    if [ $environment == "dev" ]
    then
        cd ansible/ && vagrant status && cd ..
    else
        # Figure how to show production/staging RabbitMQ, Postgres and NAT nodes
        echo "listing staging/production nodes"
    fi
    echo
    echo "Nodes"
    kubectl get nodes -o wide
    kubectl top nodes
    echo
    echo "Pods"
    kubectl get pods -o wide
    kubectl top pods
    echo
    echo "Services"
    kubectl get svc
    echo
    echo "Deployments"
    kubectl get deployments
    echo
    echo "Horizontal Autoscale stats"
    kubectl get hpa
    echo
    echo "Available namespaces"
    kubectl get namespaces
}

qa_logs(){
    if [ $# -lt 1 ]
    then
        echo "'$0 $environment logs' requires a pod (check 'list' command)"
        echo
        qa_help
        exit 1
    fi
    echo "Showing logs for $environment"
    kubectl logs "$@"
}

qa_k(){
    echo "Running kubectl on $environment"
    kubectl "$@"
}

qa_ssh() {
    if [ $# -lt 1 ]
    then
        echo "'$0 $environment ssh' requires a node or pod (check 'list' command)"
        echo
        qa_help
        exit 1
    fi

    echo "SSH'ing in $environment/$1"

    # Check if it's pod or node
    resource=$1
    if [[ $resource =~ "deployment" ]]
    then
        echo "SSH'ing into $environment pod"
        kubectl exec -it $resource -- bash
    else
        echo "SSH'ing into $environment node"
        if [ $environment == "dev" ]
        then
            ssh -F ansible/.vagrant/ssh_config -o StrictHostKeyChecking=no $resource
        else
            # Figure how to show production/staging RabbitMQ, Postgres and NAT nodes
            echo "ssh $resource"
        fi
    fi
}

qa_dashboard() {
    echo "Bringing kubernetes-dashboard for $environment: not yet implemented :)"
}

if [ $# -lt 1 ]
then
    qa_help
    exit 1
fi


environment=$1
export KUBECONFIG="./kubeconfig.$environment"
shift
re="^($available_envs)$"
if [[ ! $environment =~ $re ]]
then
    echo "$environment does not match any of $available_envs"
    qa_help
    exit 1
fi


if [ $# -lt 1 ]
then
    echo "'command' is missing"
    echo
    qa_help
    exit 1
fi


command=$1
command_exists=$(grep -c qa_$command $0 || :)
shift
if [ $command_exists == 0 ]
then
    echo "'$command' does not match any valid command"
    echo
    qa_help
    exit 1
fi

command=qa_$command
$command "$@"
