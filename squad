#!/bin/bash

set -eu


export PATH="./bin:$PATH"
available_envs="dev|staging|production"


check_dep () {
    if ! which "$1" > /dev/null
    then
      echo "Could not find $1! Please install using your package manager or place its binary in './bin'"
      exit 1
    fi
}


# Check if all tools are installed, do not go on if any is missing
for tool in terraform ansible kubectl
do
    check_dep $tool
done


squad_help() {
    echo "This is squad' deploy management tool."
    echo
    echo "Usage: ./squad $available_envs [command[args]]"
    echo
    echo "Commands:"
    echo "  up             create the entire setup"
    echo "  destroy        destroy the entire setup"
    echo "  list           show all services (e.g. frontend, workers, etc)"
    echo "  upgrade        upgrade the entire setup"
    echo "  ssh node|pod   ssh into a node or a pod"
    echo "  logs [-f] pod  display logs for a given pod, pass -f to keep following"
    echo "  dashboard      pop up a kubernetes dashboard"
    echo "  k              runs kubectl for k8s specific commands"
    echo
    echo "Example:"
    echo "  $ ./squad production upgrade  # probably the most frequent command"
}

squad_up() {
    echo "Creating $environment"

    if [ $environment == "dev" ]
    then
        echo "It should take a few minutes, after that you should be able to see squad in action at http://192.168.50.10"
        # Set up 2 virtubalboxes and install kubernetes
        check_dep vagrant
        cd ansible
        vagrant up
        ./deploy
        cd ..

        # Get generated kubeconfig
        scp \
            -F ansible/.vagrant/ssh_config \
            -o StrictHostKeyChecking=no \
            master-node:/home/vagrant/.kube/config $KUBECONFIG 

        # Prepare k8s environment
        kubectl apply -f k8s/squad-dev-namespace.yml
        kubectl apply -f k8s/metrics-server-dev.yml
    else
        echo "Creating terraform dependencies for $environment"
        cd terraform
        echo "Shared resources need to be managed first"
        ./terraform shared apply
        echo
        echo "Done"
        echo
        echo "${environment} resources are about to be managed"
        ./terraform $environment apply
        #k apply -f k8s/metrics-server-aws.yml  # fetch metrics from k8s API to feed autoscalers
        #k apply -f k8s/squad-namespaces.yml  # start job to apply migration, delete when finish
    fi

    sleep $((60 * 2))  # wait about 2 minutes for kube-system pods to get started
    squad_deploy_app
    squad_list
}

squad_destroy() {
    echo "Destroying $environment"
    if [ $environment == "dev" ]
    then
        # Set up 2 virtubalboxes and install kubernetes
        check_dep vagrant
        rm -rf $KUBECONFIG ansible/.vagrant/ssh_config
        cd ansible
        vagrant destroy -f
    else
        # Destroy production/staging
        echo "Wait 5 minutes to let EKS cluster to finish destroying"
        sleep $((60 * 5))
        cd terraform
        ./terraform ${environment} destroy
        ./terraform shared destroy
    fi
}

squad_migrate_db() {
    kubectl apply -f k8s/squad-migration.yml  # start job to apply migration, delete when finish
    kubectl wait --for=condition=complete --timeout=300s job/squad-migration  # Wait 5 minutes for migration to complete
    if [ $? != 0 ]
    then
        echo "Failed to apply migration:"
        kubectl logs job/squad-migration
        kubectl delete -f k8s/squad-migration.yml  # delete failed migration job
        echo "*** ABORTING !!! ***"
        exit 1
    fi

    kubectl delete -f k8s/squad-migration.yml  # delete completed migration job
}

squad_upgrade() {
    echo "Upgraging $environment"

    squad_deploy_app
    # Check these commands later
    # kubectl set image deployment/frontend www=image:v2               # Rolling update "www" containers of "frontend" deployment, updating the image
    # kubectl rollout history deployment/frontend                      # Check the history of deployments including the revision
    # kubectl rollout undo deployment/frontend                         # Rollback to the previous deployment
    # kubectl rollout undo deployment/frontend --to-revision=2         # Rollback to a specific revision
    # kubectl rollout status -w deployment/frontend                    # Watch rolling update status of "frontend" deployment until completion
    # kubectl rollout restart deployment/frontend                      # Rolling restart of the "frontend" deployment`
}

squad_deploy_app() {
    echo "Deploying $environment"

    kubectl config set-context --current --namespace=squad-$environment

    # set secrets/environment variables
    if [ $environment == "dev" ]
    then
        # TODO: merge all dev files into a single one
        kubectl apply -f k8s/squad-secrets-dev.yml
        kubectl apply -f k8s/squad-ldap-dev.yml
    else
        # TODO: create a "vars" file for prod and staging, then translate them into
        # k8s secrets
        ./scripts/ansible-vault view secrets/${environment}.env | ./scripts/env2secrets.py | kubectl apply -f -
        ./scripts/ansible-vault view secrets/squad-ldap.yml | kubectl apply -f -
        ./scripts/ansible-vault view secrets/squad-secret-key.yml | kubectl apply -f -
    fi
    squad_migrate_db
    kubectl apply -f k8s/squad-web.yml                   # startup frontend deploy
    kubectl apply -f k8s/squad-worker.yml                # startup worker (no fetching) deploy
    kubectl apply -f k8s/squad-listener.yml              # startup listener deploy
    kubectl apply -f k8s/squad-scheduler.yml             # startup scheduler deploy
    kubectl apply -f k8s/squad-fetch-worker.yml          # startup fetch worker deploy
}

squad_list() {
    echo "Showing services for $environment"
    echo
    echo "Nodes"
    kubectl get nodes -o wide
    kubectl top nodes
    echo
    echo "Pods"
    kubectl get pods -o wide
    kubectl top pods
    echo
    echo "Services"
    kubectl get svc
    echo
    echo "Deployments"
    kubectl get deployments
    echo
    echo "Horizontal Autoscale stats"
    kubectl get hpa
    echo
    echo "Available namespaces"
    kubectl get namespaces
}

squad_logs(){
    if [ $# -lt 1 ]
    then
        echo "'$0 $environment logs' requires a pod (check 'list' command)"
        echo
        squad_help
        exit 1
    fi
    echo "Showing logs for $environment"
    kubectl logs "$@"
}

squad_k(){
    echo "Running kubectl on $environment"
    kubectl "$@"
}

squad_ssh() {
    if [ $# -lt 1 ]
    then
        echo "'$0 $environment ssh' requires a node or pod (check 'list' command)"
        echo
        squad_help
        exit 1
    fi

    echo "SSH'ing in $environment/$1"

    # Check if it's pod or node
    resource=$1
    if [[ $resource =~ "deployment" ]]
    then
        echo "SSH'ing into $environment pod"
        kubectl exec -it $resource -- bash
    else
        echo "SSH'ing into $environment node"
        if [ $environment == "dev" ]
        then
            ssh -F ansible/.vagrant/ssh_config -o StrictHostKeyChecking=no $resource
        else
            # Figure how to show production/staging RabbitMQ, Postgres and NAT nodes
            echo "ssh $resource"
        fi
    fi
}

squad_dashboard() {
    echo "Bringing kubernetes-dashboard for $environment: not yet implemented :)"
}

if [ $# -lt 1 ]
then
    squad_help
    exit 1
fi


environment=$1
shift
re="^($available_envs)$"
if [[ ! $environment =~ $re ]]
then
    echo "$environment does not match any of $available_envs"
    squad_help
    exit 1
fi


# Set kubeconfig for kubectl to work properly
if [ $environment == "dev" ]
then
    export KUBECONFIG="./kubeconfig"
else
    export KUBECONFIG="./terraform/generated/kubeconfig"
fi


if [ $# -lt 1 ]
then
    echo "'command' is missing"
    echo
    squad_help
    exit 1
fi


command=$1
command_exists=$(grep -c squad_$command $0 || :)
shift
if [ $command_exists == 0 ]
then
    echo "'$command' does not match any valid command"
    echo
    squad_help
    exit 1
fi

command=squad_$command
$command "$@"
